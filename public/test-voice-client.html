<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>SafeEscape Voice Assistant Test</title>
  <style>
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
      background-color: #f5f5f5;
      color: #333;
    }
    h1 {
      color: #e63946;
      text-align: center;
    }
    .container {
      background: white;
      border-radius: 8px;
      padding: 20px;
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    .voice-controls {
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 20px;
      margin-bottom: 20px;
    }
    .button-row {
      display: flex;
      gap: 10px;
      justify-content: center;
    }
    button {
      padding: 12px 24px;
      border: none;
      border-radius: 50px;
      cursor: pointer;
      font-size: 16px;
      font-weight: bold;
      transition: all 0.2s ease;
    }
    button:disabled {
      opacity: 0.5;
      cursor: not-allowed;
    }
    .record-btn {
      background-color: #e63946;
      color: white;
    }
    .record-btn.recording {
      background-color: #d00000;
      animation: pulse 1.5s infinite;
    }
    .stop-btn {
      background-color: #457b9d;
      color: white;
    }
    .clear-btn {
      background-color: #1d3557;
      color: white;
    }
    .conversation {
      display: flex;
      flex-direction: column;
      gap: 15px;
      margin-top: 20px;
      max-height: 400px;
      overflow-y: auto;
      padding: 10px;
    }
    .message {
      padding: 12px;
      border-radius: 8px;
      max-width: 80%;
      line-height: 1.4;
    }
    .user {
      align-self: flex-end;
      background-color: #457b9d;
      color: white;
    }
    .assistant {
      align-self: flex-start;
      background-color: #a8dadc;
      color: #1d3557;
    }
    .status {
      text-align: center;
      font-style: italic;
      margin: 10px 0;
    }
    .vu-meter {
      width: 300px;
      height: 20px;
      background-color: #e0e0e0;
      border-radius: 10px;
      overflow: hidden;
    }
    .vu-bar {
      height: 100%;
      background-color: #2a9d8f;
      width: 0;
      transition: width 0.1s ease;
    }
    @keyframes pulse {
      0% { transform: scale(1); }
      50% { transform: scale(1.05); }
      100% { transform: scale(1); }
    }
  </style>
</head>
<body>
  <h1>SafeEscape Voice Assistant</h1>
  <div class="container">
    <div class="voice-controls">
      <div class="status">Ready to help with emergency information</div>
      <div class="vu-meter">
        <div class="vu-bar" id="vuBar"></div>
      </div>
      <div class="button-row">
        <button class="record-btn" id="recordBtn">Start Speaking</button>
        <button class="stop-btn" id="stopBtn" disabled>Stop</button>
        <button class="clear-btn" id="clearBtn">Clear Chat</button>
      </div>
    </div>
    <div class="conversation" id="conversation">
      <div class="message assistant">
        Hello! I'm the SafeEscape emergency assistant. You can ask me questions about emergencies, safety procedures, or evacuation instructions. How can I help you today?
      </div>
    </div>
  </div>

  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // DOM elements
      const recordBtn = document.getElementById('recordBtn');
      const stopBtn = document.getElementById('stopBtn');
      const clearBtn = document.getElementById('clearBtn');
      const conversation = document.getElementById('conversation');
      const statusText = document.querySelector('.status');
      const vuBar = document.getElementById('vuBar');
      
      // API endpoints
      const API_URL = window.location.origin;
      const VOICE_API = `${API_URL}/api/voice`;
      
      // Audio recording variables
      let mediaRecorder;
      let audioChunks = [];
      let isRecording = false;
      let stream;
      let audioContext;
      let analyser;
      let microphone;
      let animationFrame;
      
      // Initialize audio context
      function initAudioContext() {
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        analyser = audioContext.createAnalyser();
        analyser.fftSize = 256;
      }
      
      // Start recording
      async function startRecording() {
        try {
          stream = await navigator.mediaDevices.getUserMedia({ audio: true });
          
          if (!audioContext) {
            initAudioContext();
          }
          
          microphone = audioContext.createMediaStreamSource(stream);
          microphone.connect(analyser);
          
          // Set up media recorder
          mediaRecorder = new MediaRecorder(stream);
          audioChunks = [];
          
          mediaRecorder.ondataavailable = event => {
            audioChunks.push(event.data);
          };
          
          mediaRecorder.onstop = handleRecordingStop;
          
          // Start recording
          mediaRecorder.start(100);
          isRecording = true;
          
          // Update UI
          recordBtn.classList.add('recording');
          recordBtn.disabled = true;
          stopBtn.disabled = false;
          statusText.textContent = 'Listening...';
          
          // Start visualizing audio
          visualizeAudio();
          
          addMessage('System: Recording started...', 'assistant');
        } catch (error) {
          console.error('Error accessing microphone:', error);
          statusText.textContent = 'Error: Could not access microphone';
        }
      }
      
      // Stop recording
      function stopRecording() {
        if (mediaRecorder && isRecording) {
          mediaRecorder.stop();
          isRecording = false;
          
          // Stop visualization
          cancelAnimationFrame(animationFrame);
          
          // Update UI
          recordBtn.classList.remove('recording');
          recordBtn.disabled = false;
          stopBtn.disabled = true;
          statusText.textContent = 'Processing...';
          vuBar.style.width = '0%';
          
          // Stop all tracks
          if (stream) {
            stream.getTracks().forEach(track => track.stop());
          }
        }
      }
      
      // Handle recording stop
      async function handleRecordingStop() {
        statusText.textContent = 'Processing your request...';
        
        // Create audio blob and send to server
        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
        
        // Create FormData and add the audio blob
        const formData = new FormData();
        formData.append('audio', audioBlob);
        
        try {
          // Convert blob to base64 for API call
          const base64Audio = await blobToBase64(audioBlob);
          
          // Call API
          const response = await fetch(`${VOICE_API}/conversation`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json'
            },
            body: JSON.stringify({
              audio: base64Audio,
              audioConfig: {
                languageCode: 'en-US',
                sampleRateHertz: 48000,
                encoding: 'WEBM_OPUS'
              }
            })
          });
          
          if (response.ok) {
            // For audio/mp3 response
            const audioBlob = await response.blob();
            
            // Play audio response
            const audioUrl = URL.createObjectURL(audioBlob);
            const audio = new Audio(audioUrl);
            
            // Get text transcript from another endpoint
            const textResponse = await fetch(`${VOICE_API}/input`, {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json'
              },
              body: JSON.stringify({
                audio: base64Audio,
                audioConfig: {
                  languageCode: 'en-US',
                  sampleRateHertz: 48000,
                  encoding: 'WEBM_OPUS'
                }
              })
            });
            
            if (textResponse.ok) {
              const textData = await textResponse.json();
              
              // Add messages to conversation
              addMessage(textData.transcribedText, 'user');
              addMessage(textData.aiResponse, 'assistant');
            }
            
            // Play audio
            audio.onended = () => {
              statusText.textContent = 'Ready to help with emergency information';
            };
            
            audio.play();
            statusText.textContent = 'Playing response...';
          } else {
            console.error('Error from voice API:', await response.text());
            statusText.textContent = 'Error processing your request';
          }
        } catch (error) {
          console.error('Error processing audio:', error);
          statusText.textContent = 'Error processing audio';
        }
      }
      
      // Convert Blob to Base64
      function blobToBase64(blob) {
        return new Promise((resolve, reject) => {
          const reader = new FileReader();
          reader.onloadend = () => resolve(reader.result.split(',')[1]);
          reader.onerror = reject;
          reader.readAsDataURL(blob);
        });
      }
      
      // Visualize audio input
      function visualizeAudio() {
        if (!analyser) return;
        
        const dataArray = new Uint8Array(analyser.frequencyBinCount);
        
        const updateVisualization = () => {
          analyser.getByteFrequencyData(dataArray);
          
          // Calculate volume level (average of frequencies)
          let sum = dataArray.reduce((acc, val) => acc + val, 0);
          let avg = sum / dataArray.length;
          
          // Scale to percentage (0-100%)
          let volumePercentage = Math.min(100, Math.max(0, avg * 2));
          
          // Update VU meter
          vuBar.style.width = `${volumePercentage}%`;
          
          // Change color based on volume
          if (volumePercentage > 75) {
            vuBar.style.backgroundColor = '#e63946'; // High volume
          } else if (volumePercentage > 30) {
            vuBar.style.backgroundColor = '#2a9d8f'; // Medium volume
          } else {
            vuBar.style.backgroundColor = '#a8dadc'; // Low volume
          }
          
          if (isRecording) {
            animationFrame = requestAnimationFrame(updateVisualization);
          }
        };
        
        animationFrame = requestAnimationFrame(updateVisualization);
      }
      
      // Add message to conversation
      function addMessage(text, sender) {
        const messageDiv = document.createElement('div');
        messageDiv.classList.add('message', sender);
        messageDiv.textContent = text;
        conversation.appendChild(messageDiv);
        conversation.scrollTop = conversation.scrollHeight;
      }
      
      // Event listeners
      recordBtn.addEventListener('click', startRecording);
      stopBtn.addEventListener('click', stopRecording);
      clearBtn.addEventListener('click', () => {
        // Clear all messages except the first one
        while (conversation.children.length > 1) {
          conversation.removeChild(conversation.lastChild);
        }
      });
    });
  </script>
</body>
</html>
